{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于自注意力的Transformer实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一个基于自注意力机制的全新神经网络架构，训练速度提升了一个数量级。\n",
    "\n",
    "此文转自:[keras-attention](https://github.com/datalogue/keras-attention)，论文基础来自：[How to Visualize Your Recurrent Neural Network with Attention in Keras](https://medium.com/datalogue/attention-in-keras-1892773a4f22)\n",
    "\n",
    "这个例子的应用：将各种格式的日期文本，转为标准日期格式文本，如将26.12.18，转为2018-12-26， 将Sat 8 Jun 2017转为2017-06-08\n",
    "\n",
    "注意力机制的示意图：\n",
    "<img src='./image/attention.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现本文的例子，需要准备以下Python包：\n",
    "```\n",
    "numpy\n",
    "babel>=1.3\n",
    "tensorflow>=1.1.0\n",
    "keras>=2.0.4\n",
    "h5py\n",
    "Faker\n",
    "matplotlib>=2.0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你的电脑具有GPU运算能力，需要将tensorflow替换为：```tensorflow-gpu>=1.1.0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过faker以及babel，随机创建训练集以及验证集，其中faker可以随机生成日期，而babel是Python的一个国际化工具包，可以实现日期格式本地化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以美国区域为例，常见的日期字符串的例子：30 october 1989， 23.06.11， 3 05 18等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "from faker import Faker\n",
    "from babel.dates import format_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定自动生成数据文件所在文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './keras_attention/data'\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "fake.seed(230517)\n",
    "random.seed(230517)\n",
    "# 定义15种日期格式\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'd MMM YYY',\n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY',\n",
    "           ]\n",
    "# change this if you want it to work with only a single language\n",
    "LOCALES = ['en_US']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义随机生成日期的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date():\n",
    "    \"\"\"\n",
    "        Creates some fake dates \n",
    "        :returns: tuple containing \n",
    "                  1. human formatted string\n",
    "                  2. machine formatted string\n",
    "                  3. date object.\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    # wrapping this in a try catch because\n",
    "    # the locale 'vo' and format 'full' will fail\n",
    "    try:\n",
    "        human = format_date(dt,\n",
    "                            format=random.choice(FORMATS),\n",
    "                            locale=random.choice(LOCALES))\n",
    "\n",
    "        case_change = random.randint(0,3) # 1/2 chance of case change\n",
    "        if case_change == 1:\n",
    "            human = human.upper()\n",
    "        elif case_change == 2:\n",
    "            human = human.lower()\n",
    "\n",
    "        machine = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        # print(e)\n",
    "        return None, None, None\n",
    "\n",
    "    return human, machine, dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面测试该函数的随机日期输出，分别为手写日期字符串，标准日期字符串以及日期类型数据对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('19 July 1989', '1989-07-19', datetime.date(1989, 7, 19))\n",
      "('may 18, 1971', '1971-05-18', datetime.date(1971, 5, 18))\n",
      "('June 12, 2005', '2005-06-12', datetime.date(2005, 6, 12))\n",
      "('jul 31, 1988', '1988-07-31', datetime.date(1988, 7, 31))\n",
      "('30 march, 2017', '2017-03-30', datetime.date(2017, 3, 30))\n",
      "('March 3 1985', '1985-03-03', datetime.date(1985, 3, 3))\n",
      "('05.09.87', '1987-09-05', datetime.date(1987, 9, 5))\n",
      "('17 Jul 2005', '2005-07-17', datetime.date(2005, 7, 17))\n",
      "('MONDAY, JANUARY 4, 2010', '2010-01-04', datetime.date(2010, 1, 4))\n",
      "('OCTOBER 27 1972', '1972-10-27', datetime.date(1972, 10, 27))\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(create_date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集的方法\n",
    "\n",
    "- human_vocab，集合类型，即利用集合去重的特性，构建手写日期字符串的字符表，对应Encoder\n",
    ">如12, Sep 2010，通过tuple处理，得到：('1', '2', ',', ' ', 'S', 'e', 'p', ' ', '2', '0', '1', '0')\n",
    "<br>通过human_vocab.update(tuple('12, Sep 2010'))，完成去重得到：{'2', 'e', 'p', ',', ' ', '1', 'S', '0'}\n",
    "- machine_vocab，集合类型，即利用集合去重的特性，构建即标准化日期字符串的字符表，对应Decoder\n",
    ">如2010-09-12，通过tuple处理，得到：('2', '0', '1', '0', '-', '0', '9', '-', '1', '2')\n",
    "<br>通过machine_vocab.update(tuple('2019-09-12'))，完成去重得到：{'2', '9', '1', '0', '-'}\n",
    "- ```int2human = dict(enumerate(human_vocab))```\n",
    ">其中enumerate是对集合human_vocab做迭代器，然后通过dict进行字典显式转换，得到字典结构：{0: 'C', 1: 'N', 2: '8', 3: 'R', 4: '2', 5: 'e', 6: 'p', 7: ',', 8: 'M', 9: 'B', 10: 'E', 11: 'J', 12: 'D', 13: 'U', 14: '9', 15: ' ', 16: '1', 17: 'S', 18: '0', 19: '4'}\n",
    "- ```int2human.update({len(int2human): '<unk>',len(int2human)+1: '<eot>'})```,这个目的是设置空与结尾的字典\n",
    ">得到```{0: 'C', 1: 'N', 2: '8', 3: 'R', 4: '2', 5: 'e', 6: 'p', 7: ',', 8: 'M', 9: 'B', 10: 'E', 11: 'J', 12: 'D', 13: 'U', 14: '9', 15: ' ', 16: '1', 17: 'S', 18: '0', 19: '4', 20: '<unk>', 21: '<eot>'}```\n",
    "- 因为我们需要字符与索引的对应关系，所以字典需要颠倒一下：\n",
    ">```human2int = {v: k for k, v in int2human.items()}```\n",
    "<br>从而得到：```{'C': 0, 'N': 1, '8': 2, 'R': 3, '2': 4, 'e': 5, 'p': 6, ',': 7, 'M': 8, 'B': 9, 'E': 10, 'J': 11, 'D': 12, 'U': 13, '9': 14, ' ': 15, '1': 16, 'S': 17, '0': 18, '4': 19, '<unk>': 20, '<eot>': 21}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_name, n_examples, vocabulary=False):\n",
    "    \"\"\"\n",
    "        Creates a csv dataset with n_examples and optional vocabulary\n",
    "        :param dataset_name: name of the file to save as\n",
    "        :n_examples: the number of examples to generate\n",
    "        :vocabulary: if true, will also save the vocabulary\n",
    "    \"\"\"\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "\n",
    "    with open(dataset_name, 'w', encoding='utf-8') as f:\n",
    "        for i in range(n_examples):\n",
    "            h, m, _ = create_date()\n",
    "            if h is not None:\n",
    "                f.write('\"'+h + '\",\"' + m + '\"\\n')\n",
    "                human_vocab.update(tuple(h))\n",
    "                machine_vocab.update(tuple(m))\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print('Generated {0} records'.format(i + 1))\n",
    "\n",
    "    if vocabulary:\n",
    "        int2human = dict(enumerate(human_vocab))\n",
    "        int2human.update({len(int2human): '<unk>',\n",
    "                          len(int2human)+1: '<eot>'})\n",
    "        int2machine = dict(enumerate(machine_vocab))\n",
    "        int2machine.update({len(int2machine):'<unk>',\n",
    "                            len(int2machine)+1:'<eot>'})\n",
    "\n",
    "        human2int = {v: k for k, v in int2human.items()}\n",
    "        machine2int = {v: k for k, v in int2machine.items()}\n",
    "\n",
    "        with open(os.path.join(DATA_FOLDER, 'human_vocab.json'), 'w') as f:\n",
    "            json.dump(human2int, f)\n",
    "        with open(os.path.join(DATA_FOLDER, 'machine_vocab.json'), 'w') as f:\n",
    "            json.dump(machine2int, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./keras_attention/data\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(DATA_FOLDER)\n",
    "print(os.path.exists(DATA_FOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成训练集以及验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training dataset begin\n",
      "Generated 1000 records\n",
      "Generated 2000 records\n",
      "Generated 3000 records\n",
      "Generated 4000 records\n",
      "Generated 5000 records\n",
      "Generated 6000 records\n",
      "Generated 7000 records\n",
      "Generated 8000 records\n",
      "Generated 9000 records\n",
      "Generated 10000 records\n",
      "Generated 11000 records\n",
      "Generated 12000 records\n",
      "Generated 13000 records\n",
      "Generated 14000 records\n",
      "Generated 15000 records\n",
      "Generated 16000 records\n",
      "Generated 17000 records\n",
      "Generated 18000 records\n",
      "Generated 19000 records\n",
      "Generated 20000 records\n",
      "Generated 21000 records\n",
      "Generated 22000 records\n",
      "Generated 23000 records\n",
      "Generated 24000 records\n",
      "Generated 25000 records\n",
      "Generated 26000 records\n",
      "Generated 27000 records\n",
      "Generated 28000 records\n",
      "Generated 29000 records\n",
      "Generated 30000 records\n",
      "Generated 31000 records\n",
      "Generated 32000 records\n",
      "Generated 33000 records\n",
      "Generated 34000 records\n",
      "Generated 35000 records\n",
      "Generated 36000 records\n",
      "Generated 37000 records\n",
      "Generated 38000 records\n",
      "Generated 39000 records\n",
      "Generated 40000 records\n",
      "Generated 41000 records\n",
      "Generated 42000 records\n",
      "Generated 43000 records\n",
      "Generated 44000 records\n",
      "Generated 45000 records\n",
      "Generated 46000 records\n",
      "Generated 47000 records\n",
      "Generated 48000 records\n",
      "Generated 49000 records\n",
      "Generated 50000 records\n",
      "Generated 51000 records\n",
      "Generated 52000 records\n",
      "Generated 53000 records\n",
      "Generated 54000 records\n",
      "Generated 55000 records\n",
      "Generated 56000 records\n",
      "Generated 57000 records\n",
      "Generated 58000 records\n",
      "Generated 59000 records\n",
      "Generated 60000 records\n",
      "Generated 61000 records\n",
      "Generated 62000 records\n",
      "Generated 63000 records\n",
      "Generated 64000 records\n",
      "Generated 65000 records\n",
      "Generated 66000 records\n",
      "Generated 67000 records\n",
      "Generated 68000 records\n",
      "Generated 69000 records\n",
      "Generated 70000 records\n",
      "Generated 71000 records\n",
      "Generated 72000 records\n",
      "Generated 73000 records\n",
      "Generated 74000 records\n",
      "Generated 75000 records\n",
      "Generated 76000 records\n",
      "Generated 77000 records\n",
      "Generated 78000 records\n",
      "Generated 79000 records\n",
      "Generated 80000 records\n",
      "Generated 81000 records\n",
      "Generated 82000 records\n",
      "Generated 83000 records\n",
      "Generated 84000 records\n",
      "Generated 85000 records\n",
      "Generated 86000 records\n",
      "Generated 87000 records\n",
      "Generated 88000 records\n",
      "Generated 89000 records\n",
      "Generated 90000 records\n",
      "Generated 91000 records\n",
      "Generated 92000 records\n",
      "Generated 93000 records\n",
      "Generated 94000 records\n",
      "Generated 95000 records\n",
      "Generated 96000 records\n",
      "Generated 97000 records\n",
      "Generated 98000 records\n",
      "Generated 99000 records\n",
      "Generated 100000 records\n",
      "Generated 101000 records\n",
      "Generated 102000 records\n",
      "Generated 103000 records\n",
      "Generated 104000 records\n",
      "Generated 105000 records\n",
      "Generated 106000 records\n",
      "Generated 107000 records\n",
      "Generated 108000 records\n",
      "Generated 109000 records\n",
      "Generated 110000 records\n",
      "Generated 111000 records\n",
      "Generated 112000 records\n",
      "Generated 113000 records\n",
      "Generated 114000 records\n",
      "Generated 115000 records\n",
      "Generated 116000 records\n",
      "Generated 117000 records\n",
      "Generated 118000 records\n",
      "Generated 119000 records\n",
      "Generated 120000 records\n",
      "Generated 121000 records\n",
      "Generated 122000 records\n",
      "Generated 123000 records\n",
      "Generated 124000 records\n",
      "Generated 125000 records\n",
      "Generated 126000 records\n",
      "Generated 127000 records\n",
      "Generated 128000 records\n",
      "Generated 129000 records\n",
      "Generated 130000 records\n",
      "Generated 131000 records\n",
      "Generated 132000 records\n",
      "Generated 133000 records\n",
      "Generated 134000 records\n",
      "Generated 135000 records\n",
      "Generated 136000 records\n",
      "Generated 137000 records\n",
      "Generated 138000 records\n",
      "Generated 139000 records\n",
      "Generated 140000 records\n",
      "Generated 141000 records\n",
      "Generated 142000 records\n",
      "Generated 143000 records\n",
      "Generated 144000 records\n",
      "Generated 145000 records\n",
      "Generated 146000 records\n",
      "Generated 147000 records\n",
      "Generated 148000 records\n",
      "Generated 149000 records\n",
      "Generated 150000 records\n",
      "Generated 151000 records\n",
      "Generated 152000 records\n",
      "Generated 153000 records\n",
      "Generated 154000 records\n",
      "Generated 155000 records\n",
      "Generated 156000 records\n",
      "Generated 157000 records\n",
      "Generated 158000 records\n",
      "Generated 159000 records\n",
      "Generated 160000 records\n",
      "Generated 161000 records\n",
      "Generated 162000 records\n",
      "Generated 163000 records\n",
      "Generated 164000 records\n",
      "Generated 165000 records\n",
      "Generated 166000 records\n",
      "Generated 167000 records\n",
      "Generated 168000 records\n",
      "Generated 169000 records\n",
      "Generated 170000 records\n",
      "Generated 171000 records\n",
      "Generated 172000 records\n",
      "Generated 173000 records\n",
      "Generated 174000 records\n",
      "Generated 175000 records\n",
      "Generated 176000 records\n",
      "Generated 177000 records\n",
      "Generated 178000 records\n",
      "Generated 179000 records\n",
      "Generated 180000 records\n",
      "Generated 181000 records\n",
      "Generated 182000 records\n",
      "Generated 183000 records\n",
      "Generated 184000 records\n",
      "Generated 185000 records\n",
      "Generated 186000 records\n",
      "Generated 187000 records\n",
      "Generated 188000 records\n",
      "Generated 189000 records\n",
      "Generated 190000 records\n",
      "Generated 191000 records\n",
      "Generated 192000 records\n",
      "Generated 193000 records\n",
      "Generated 194000 records\n",
      "Generated 195000 records\n",
      "Generated 196000 records\n",
      "Generated 197000 records\n",
      "Generated 198000 records\n",
      "Generated 199000 records\n",
      "Generated 200000 records\n",
      "Generated 201000 records\n",
      "Generated 202000 records\n",
      "Generated 203000 records\n",
      "Generated 204000 records\n",
      "Generated 205000 records\n",
      "Generated 206000 records\n",
      "Generated 207000 records\n",
      "Generated 208000 records\n",
      "Generated 209000 records\n",
      "Generated 210000 records\n",
      "Generated 211000 records\n",
      "Generated 212000 records\n",
      "Generated 213000 records\n",
      "Generated 214000 records\n",
      "Generated 215000 records\n",
      "Generated 216000 records\n",
      "Generated 217000 records\n",
      "Generated 218000 records\n",
      "Generated 219000 records\n",
      "Generated 220000 records\n",
      "Generated 221000 records\n",
      "Generated 222000 records\n",
      "Generated 223000 records\n",
      "Generated 224000 records\n",
      "Generated 225000 records\n",
      "Generated 226000 records\n",
      "Generated 227000 records\n",
      "Generated 228000 records\n",
      "Generated 229000 records\n",
      "Generated 230000 records\n",
      "Generated 231000 records\n",
      "Generated 232000 records\n",
      "Generated 233000 records\n",
      "Generated 234000 records\n",
      "Generated 235000 records\n",
      "Generated 236000 records\n",
      "Generated 237000 records\n",
      "Generated 238000 records\n",
      "Generated 239000 records\n",
      "Generated 240000 records\n",
      "Generated 241000 records\n",
      "Generated 242000 records\n",
      "Generated 243000 records\n",
      "Generated 244000 records\n",
      "Generated 245000 records\n",
      "Generated 246000 records\n",
      "Generated 247000 records\n",
      "Generated 248000 records\n",
      "Generated 249000 records\n",
      "Generated 250000 records\n",
      "Generated 251000 records\n",
      "Generated 252000 records\n",
      "Generated 253000 records\n",
      "Generated 254000 records\n",
      "Generated 255000 records\n",
      "Generated 256000 records\n",
      "Generated 257000 records\n",
      "Generated 258000 records\n",
      "Generated 259000 records\n",
      "Generated 260000 records\n",
      "Generated 261000 records\n",
      "Generated 262000 records\n",
      "Generated 263000 records\n",
      "Generated 264000 records\n",
      "Generated 265000 records\n",
      "Generated 266000 records\n",
      "Generated 267000 records\n",
      "Generated 268000 records\n",
      "Generated 269000 records\n",
      "Generated 270000 records\n",
      "Generated 271000 records\n",
      "Generated 272000 records\n",
      "Generated 273000 records\n",
      "Generated 274000 records\n",
      "Generated 275000 records\n",
      "Generated 276000 records\n",
      "Generated 277000 records\n",
      "Generated 278000 records\n",
      "Generated 279000 records\n",
      "Generated 280000 records\n",
      "Generated 281000 records\n",
      "Generated 282000 records\n",
      "Generated 283000 records\n",
      "Generated 284000 records\n",
      "Generated 285000 records\n",
      "Generated 286000 records\n",
      "Generated 287000 records\n",
      "Generated 288000 records\n",
      "Generated 289000 records\n",
      "Generated 290000 records\n",
      "Generated 291000 records\n",
      "Generated 292000 records\n",
      "Generated 293000 records\n",
      "Generated 294000 records\n",
      "Generated 295000 records\n",
      "Generated 296000 records\n",
      "Generated 297000 records\n",
      "Generated 298000 records\n",
      "Generated 299000 records\n",
      "Generated 300000 records\n",
      "Generated 301000 records\n",
      "Generated 302000 records\n",
      "Generated 303000 records\n",
      "Generated 304000 records\n",
      "Generated 305000 records\n",
      "Generated 306000 records\n",
      "Generated 307000 records\n",
      "Generated 308000 records\n",
      "Generated 309000 records\n",
      "Generated 310000 records\n",
      "Generated 311000 records\n",
      "Generated 312000 records\n",
      "Generated 313000 records\n",
      "Generated 314000 records\n",
      "Generated 315000 records\n",
      "Generated 316000 records\n",
      "Generated 317000 records\n",
      "Generated 318000 records\n",
      "Generated 319000 records\n",
      "Generated 320000 records\n",
      "Generated 321000 records\n",
      "Generated 322000 records\n",
      "Generated 323000 records\n",
      "Generated 324000 records\n",
      "Generated 325000 records\n",
      "Generated 326000 records\n",
      "Generated 327000 records\n",
      "Generated 328000 records\n",
      "Generated 329000 records\n",
      "Generated 330000 records\n",
      "Generated 331000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 332000 records\n",
      "Generated 333000 records\n",
      "Generated 334000 records\n",
      "Generated 335000 records\n",
      "Generated 336000 records\n",
      "Generated 337000 records\n",
      "Generated 338000 records\n",
      "Generated 339000 records\n",
      "Generated 340000 records\n",
      "Generated 341000 records\n",
      "Generated 342000 records\n",
      "Generated 343000 records\n",
      "Generated 344000 records\n",
      "Generated 345000 records\n",
      "Generated 346000 records\n",
      "Generated 347000 records\n",
      "Generated 348000 records\n",
      "Generated 349000 records\n",
      "Generated 350000 records\n",
      "Generated 351000 records\n",
      "Generated 352000 records\n",
      "Generated 353000 records\n",
      "Generated 354000 records\n",
      "Generated 355000 records\n",
      "Generated 356000 records\n",
      "Generated 357000 records\n",
      "Generated 358000 records\n",
      "Generated 359000 records\n",
      "Generated 360000 records\n",
      "Generated 361000 records\n",
      "Generated 362000 records\n",
      "Generated 363000 records\n",
      "Generated 364000 records\n",
      "Generated 365000 records\n",
      "Generated 366000 records\n",
      "Generated 367000 records\n",
      "Generated 368000 records\n",
      "Generated 369000 records\n",
      "Generated 370000 records\n",
      "Generated 371000 records\n",
      "Generated 372000 records\n",
      "Generated 373000 records\n",
      "Generated 374000 records\n",
      "Generated 375000 records\n",
      "Generated 376000 records\n",
      "Generated 377000 records\n",
      "Generated 378000 records\n",
      "Generated 379000 records\n",
      "Generated 380000 records\n",
      "Generated 381000 records\n",
      "Generated 382000 records\n",
      "Generated 383000 records\n",
      "Generated 384000 records\n",
      "Generated 385000 records\n",
      "Generated 386000 records\n",
      "Generated 387000 records\n",
      "Generated 388000 records\n",
      "Generated 389000 records\n",
      "Generated 390000 records\n",
      "Generated 391000 records\n",
      "Generated 392000 records\n",
      "Generated 393000 records\n",
      "Generated 394000 records\n",
      "Generated 395000 records\n",
      "Generated 396000 records\n",
      "Generated 397000 records\n",
      "Generated 398000 records\n",
      "Generated 399000 records\n",
      "Generated 400000 records\n",
      "Generated 401000 records\n",
      "Generated 402000 records\n",
      "Generated 403000 records\n",
      "Generated 404000 records\n",
      "Generated 405000 records\n",
      "Generated 406000 records\n",
      "Generated 407000 records\n",
      "Generated 408000 records\n",
      "Generated 409000 records\n",
      "Generated 410000 records\n",
      "Generated 411000 records\n",
      "Generated 412000 records\n",
      "Generated 413000 records\n",
      "Generated 414000 records\n",
      "Generated 415000 records\n",
      "Generated 416000 records\n",
      "Generated 417000 records\n",
      "Generated 418000 records\n",
      "Generated 419000 records\n",
      "Generated 420000 records\n",
      "Generated 421000 records\n",
      "Generated 422000 records\n",
      "Generated 423000 records\n",
      "Generated 424000 records\n",
      "Generated 425000 records\n",
      "Generated 426000 records\n",
      "Generated 427000 records\n",
      "Generated 428000 records\n",
      "Generated 429000 records\n",
      "Generated 430000 records\n",
      "Generated 431000 records\n",
      "Generated 432000 records\n",
      "Generated 433000 records\n",
      "Generated 434000 records\n",
      "Generated 435000 records\n",
      "Generated 436000 records\n",
      "Generated 437000 records\n",
      "Generated 438000 records\n",
      "Generated 439000 records\n",
      "Generated 440000 records\n",
      "Generated 441000 records\n",
      "Generated 442000 records\n",
      "Generated 443000 records\n",
      "Generated 444000 records\n",
      "Generated 445000 records\n",
      "Generated 446000 records\n",
      "Generated 447000 records\n",
      "Generated 448000 records\n",
      "Generated 449000 records\n",
      "Generated 450000 records\n",
      "Generated 451000 records\n",
      "Generated 452000 records\n",
      "Generated 453000 records\n",
      "Generated 454000 records\n",
      "Generated 455000 records\n",
      "Generated 456000 records\n",
      "Generated 457000 records\n",
      "Generated 458000 records\n",
      "Generated 459000 records\n",
      "Generated 460000 records\n",
      "Generated 461000 records\n",
      "Generated 462000 records\n",
      "Generated 463000 records\n",
      "Generated 464000 records\n",
      "Generated 465000 records\n",
      "Generated 466000 records\n",
      "Generated 467000 records\n",
      "Generated 468000 records\n",
      "Generated 469000 records\n",
      "Generated 470000 records\n",
      "Generated 471000 records\n",
      "Generated 472000 records\n",
      "Generated 473000 records\n",
      "Generated 474000 records\n",
      "Generated 475000 records\n",
      "Generated 476000 records\n",
      "Generated 477000 records\n",
      "Generated 478000 records\n",
      "Generated 479000 records\n",
      "Generated 480000 records\n",
      "Generated 481000 records\n",
      "Generated 482000 records\n",
      "Generated 483000 records\n",
      "Generated 484000 records\n",
      "Generated 485000 records\n",
      "Generated 486000 records\n",
      "Generated 487000 records\n",
      "Generated 488000 records\n",
      "Generated 489000 records\n",
      "Generated 490000 records\n",
      "Generated 491000 records\n",
      "Generated 492000 records\n",
      "Generated 493000 records\n",
      "Generated 494000 records\n",
      "Generated 495000 records\n",
      "Generated 496000 records\n",
      "Generated 497000 records\n",
      "Generated 498000 records\n",
      "Generated 499000 records\n",
      "Generated 500000 records\n",
      "creating training dataset end\n",
      "\n",
      "creating validation dataset begin\n",
      "Generated 1000 records\n",
      "creating validation dataset end\n"
     ]
    }
   ],
   "source": [
    "print('creating training dataset begin')\n",
    "create_dataset(os.path.join(DATA_FOLDER, 'training.csv'), 500000,\n",
    "               vocabulary=True)\n",
    "print('creating training dataset end')\n",
    "print()\n",
    "print('creating validation dataset begin')\n",
    "create_dataset(os.path.join(DATA_FOLDER, 'validation.csv'), 1000)\n",
    "print('creating validation dataset end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的数据文件："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/attentiondata.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成数据的例子：\n",
    "```\n",
    "\"9/30/97\",\"1997-09-30\"\n",
    "\"November 30, 1986\",\"1986-11-30\"\n",
    "\"25.05.78\",\"1978-05-25\"\n",
    "\"23.05.72\",\"1972-05-23\"\n",
    "\"TUESDAY, MAY 20, 1975\",\"1975-05-20\"\n",
    "\"27 Jul 1995\",\"1995-07-27\"\n",
    "\"Sunday, April 18, 1999\",\"1999-04-18\"\n",
    "\"july 15 1986\",\"1986-07-15\"\n",
    "\"19 Nov 1979\",\"1979-11-19\"\n",
    "\"JULY 12, 2001\",\"2001-07-12\"\n",
    "\"February 25 2018\",\"2018-02-25\"\n",
    "\"10.02.70\",\"1970-02-10\"\n",
    "\"June 5 1975\",\"1975-06-05\"\n",
    "\"01 Jun 1977\",\"1977-06-01\"\n",
    "\"dec 13, 2006\",\"2006-12-13\"\n",
    "\"9 DEC, 1995\",\"1995-12-09\"\n",
    "\"august 5, 1972\",\"1972-08-05\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定储存模型文件的文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_FOLDER = './keras_attention/weights'\n",
    "if not os.path.exists(WEIGHT_FOLDER):\n",
    "    os.makedirs(WEIGHT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备构建基于Attention机制模型的各种方法与类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于理解，此篇文章将涉及的类与方法，都直接放置到本篇文章中，而不是作为单独的python文件存放。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文的目录结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/attentionstructure.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关的模块与类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.engine import InputSpec\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Activation, Permute, Input, Flatten, Dropout\n",
    "from keras.layers.recurrent import LSTM, Recurrent\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每一个临时切片都做一次: 输入x与权重w的矩阵乘法+偏移量b，有些类似于做线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `x . w + b` for every temporal slice.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention机制的Decoder类：AttentionDecoder，其继承了RNN的父类：Recurrent, 实现这个类之后，即可作为神经网络中的Attention层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>重要构造函数介绍</color></b>\n",
    "- activation,激活函数: 这里选用了tanh\n",
    "- kernel_initializer，权值初始化方法， 用于初始化权值的初始化器\n",
    ">这里选用了glorot_uniform, Glorot均匀分布初始化方法，又称Xavier均匀初始化，目标就是使得每一层输出的方差应该尽量相等。参数从```[-limit, limit]```的均匀分布产生，其中limit为```sqrt(6 / (fan_in + fan_out))```。\n",
    "<br>有篇很好的文章:[深度学习中Xavier初始化](https://www.cnblogs.com/hejunlin1992/p/8723816.html),可以通过如下方式计算：\n",
    "<br>```scale = np.sqrt(6. / (shape[0] + shape[1])) \n",
    "np.random.uniform(low=-scale, high=scale, size=shape)```\n",
    "更多TensorFlow的参数初始化方法，参见:[tensorflow 1.0 学习：参数初始化（initializer)](http://www.mamicode.com/info-detail-1835147.html)\n",
    "- recurrent_initializer，循环层状态节点权重初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的函数。\n",
    ">这里选择了orthogonal，正交方法初始化(orthogonal initialization)网络参数，可以参考这篇博客[Explaining and illustrating orthogonal initialization for recurrent neural networks](https://smerity.com/articles/2016/orthogonal_init.html)\n",
    "><br><b>正交初始化：</b>\n",
    "<br>理想的情况是，特征值绝对值为1。则无论步数增加多少，梯度都在数值计算的精度内。\n",
    "<br>这样的参数矩阵W是单位正交阵。\n",
    "<br><b>把转移矩阵初始化为单位正交阵，可以避免在训练一开始就发生梯度爆炸/消失现象，称为orthogonal initialization。</b>\n",
    "- bias_initializer, 偏移值初始化，这里设置的为zeros，即0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "class AttentionDecoder(Recurrent):\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states \n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \n",
    "            \"Neural machine translation by jointly learning to align and translate.\" \n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        print('inputs shape:', inputs.get_shape())\n",
    "\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面尝试用AttentionDecoder类，构建一个神经网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 128)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 104)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 128)          86528     \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 100, 4)            25780     \n",
      "=================================================================\n",
      "Total params: 112,308\n",
      "Trainable params: 112,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "i = Input(shape=(100,104), dtype='float32')\n",
    "enc = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(i)\n",
    "dec = AttentionDecoder(32, 4)(enc)\n",
    "model = Model(inputs=i, outputs=dec)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过Attention机制创建神经网络机器翻译（Neural Machine Translator，简称NMT）的方法，相比以往构建BI-LSTM的方式，区别就是叠加了AttentionDecoder层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleNMT(pad_length=100,\n",
    "              n_chars=105,\n",
    "              n_labels=6,\n",
    "              embedding_learnable=False,\n",
    "              encoder_units=256,\n",
    "              decoder_units=256,\n",
    "              trainable=True,\n",
    "              return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Builds a Neural Machine Translator that has alignment attention\n",
    "    :param pad_length: the size of the input sequence\n",
    "    :param n_chars: the number of characters in the vocabulary\n",
    "    :param n_labels: the number of possible labelings for each character\n",
    "    :param embedding_learnable: decides if the one hot embedding should be refinable.\n",
    "    :return: keras.models.Model that can be compiled and fit'ed\n",
    "\n",
    "    *** REFERENCES ***\n",
    "    Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \n",
    "    \"Neural Machine Translation By Jointly Learning To Align and Translate\" \n",
    "    \"\"\"\n",
    "    input_ = Input(shape=(pad_length,), dtype='float32')\n",
    "    input_embed = Embedding(n_chars, n_chars,\n",
    "                            input_length=pad_length,\n",
    "                            trainable=embedding_learnable,\n",
    "                            weights=[np.eye(n_chars)],\n",
    "                            name='OneHot')(input_)\n",
    "\n",
    "    rnn_encoded = Bidirectional(LSTM(encoder_units, return_sequences=True),\n",
    "                                name='bidirectional_1',\n",
    "                                merge_mode='concat',\n",
    "                                trainable=trainable)(input_embed)\n",
    "\n",
    "    y_hat = AttentionDecoder(decoder_units,\n",
    "                             name='attention_decoder_1',\n",
    "                             output_dim=n_labels,\n",
    "                             return_probabilities=return_probabilities,\n",
    "                             trainable=trainable)(rnn_encoded)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=y_hat)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试一下，并观察其输出，超参数都使用默认值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 512)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "OneHot (Embedding)           (None, 100, 105)          11025     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 512)          741376    \n",
      "_________________________________________________________________\n",
      "attention_decoder_1 (Attenti (None, 100, 6)            928042    \n",
      "=================================================================\n",
      "Total params: 1,680,443\n",
      "Trainable params: 1,669,418\n",
      "Non-trainable params: 11,025\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = simpleNMT()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算Accuracy的方法，在模型编译时，用于测量准确率的metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def all_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        All Accuracy\n",
    "        https://github.com/rasmusbergpalm/normalization/blob/master/train.py#L10\n",
    "    \"\"\"\n",
    "    return K.mean(\n",
    "        K.all(\n",
    "            K.equal(\n",
    "                K.max(y_true, axis=-1),\n",
    "                K.cast(K.argmax(y_pred, axis=-1), K.floatx())\n",
    "            ),\n",
    "            axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取词汇表的类与方法（用于模型训练以及结果输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1984)\n",
    "\n",
    "INPUT_PADDING = 50\n",
    "OUTPUT_PADDING = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary类\n",
    "- vocabulary：字典，字符为key，索引为value\n",
    "- reverse_vocabulary：字典，索引为key，字符为value\n",
    "从文件创建词汇表，提供字符转整数索引，也提供索引转字符的方法\n",
    "\n",
    "无论是输入还是输出词汇表，都可以通过此类进行转换处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, vocabulary_file, padding=None):\n",
    "        \"\"\"\n",
    "            Creates a vocabulary from a file\n",
    "            :param vocabulary_file: the path to the vocabulary\n",
    "        \"\"\"\n",
    "        self.vocabulary_file = vocabulary_file\n",
    "        with open(vocabulary_file, 'r', encoding='utf-8') as f:\n",
    "            self.vocabulary = json.load(f)\n",
    "\n",
    "        self.padding = padding\n",
    "        self.reverse_vocabulary = {v: k for k, v in self.vocabulary.items()}\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "            Gets the size of the vocabulary\n",
    "        \"\"\"\n",
    "        return len(self.vocabulary.keys())\n",
    "\n",
    "    def string_to_int(self, text):\n",
    "        \"\"\"\n",
    "            Converts a string into it's character integer \n",
    "            representation\n",
    "            :param text: text to convert\n",
    "        \"\"\"\n",
    "        characters = list(text)\n",
    "\n",
    "        integers = []\n",
    "\n",
    "        if self.padding and len(characters) >= self.padding:\n",
    "            # truncate if too long\n",
    "            characters = characters[:self.padding - 1]\n",
    "\n",
    "        characters.append('<eot>')\n",
    "\n",
    "        for c in characters:\n",
    "            if c in self.vocabulary:\n",
    "                integers.append(self.vocabulary[c])\n",
    "            else:\n",
    "                integers.append(self.vocabulary['<unk>'])\n",
    "\n",
    "\n",
    "        # pad:\n",
    "        if self.padding and len(integers) < self.padding:\n",
    "            integers.extend([self.vocabulary['<unk>']]\n",
    "                            * (self.padding - len(integers)))\n",
    "\n",
    "        if len(integers) != self.padding:\n",
    "            print(text)\n",
    "            raise AttributeError('Length of text was not padding.')\n",
    "        return integers\n",
    "\n",
    "    def int_to_string(self, integers):\n",
    "        \"\"\"\n",
    "            Decodes a list of integers\n",
    "            into it's string representation\n",
    "        \"\"\"\n",
    "        characters = []\n",
    "        for i in integers:\n",
    "            characters.append(self.reverse_vocabulary[i])\n",
    "\n",
    "        return characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data类，创建从文件获取数据的对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "\n",
    "    def __init__(self, file_name, input_vocabulary, output_vocabulary):\n",
    "        \"\"\"\n",
    "            Creates an object that gets data from a file\n",
    "            :param file_name: name of the file to read from\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_vocabulary = input_vocabulary\n",
    "        self.output_vocabulary = output_vocabulary\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "            Loads data from a file\n",
    "        \"\"\"\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        with open(self.file_name, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                self.inputs.append(row[0])\n",
    "                self.targets.append(row[1])\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "            Transforms the data as necessary\n",
    "        \"\"\"\n",
    "        # @TODO: use `pool.map_async` here?\n",
    "        self.inputs = np.array(list(\n",
    "            map(self.input_vocabulary.string_to_int, self.inputs)))\n",
    "        self.targets = map(self.output_vocabulary.string_to_int, self.targets)\n",
    "        self.targets = np.array(\n",
    "            list(map(\n",
    "                lambda x: to_categorical(\n",
    "                    x,\n",
    "                    num_classes=self.output_vocabulary.size()),\n",
    "                self.targets)))\n",
    "\n",
    "        assert len(self.inputs.shape) == 2, 'Inputs could not properly be encoded'\n",
    "        assert len(self.targets.shape) == 3, 'Targets could not properly be encoded'\n",
    "\n",
    "    def generator(self, batch_size):\n",
    "        \"\"\"\n",
    "            Creates a generator that can be used in `model.fit_generator()`\n",
    "            Batches are generated randomly.\n",
    "            :param batch_size: the number of instances to include per batch\n",
    "        \"\"\"\n",
    "        instance_id = range(len(self.inputs))\n",
    "        while True:\n",
    "            try:\n",
    "                batch_ids = random.sample(instance_id, batch_size)\n",
    "                yield (np.array(self.inputs[batch_ids], dtype=int),\n",
    "                       np.array(self.targets[batch_ids]))\n",
    "            except Exception as e:\n",
    "                print('EXCEPTION OMG')\n",
    "                print(e)\n",
    "                yield None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据转换类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中经过Data类load方法处理之后，手写日期格式会作为inputs的列表成员，标准日期格式会作为targets的列表成员\n",
    "<img src='./image/attentiondata1.png' />\n",
    "<img src='./image/attentiondata2.png' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过transform之后，inputs变成了字符索引映射矩阵，从而可以成为训练集的X，即特征数据：\n",
    "<br>50列，是因为padding长度为50,1000行，是数据records数量。\n",
    "<img src='./image/attentioninputt.gif' />\n",
    "经过transform之后，targets变成了one-hot编码的矩阵，从而可以成为训练集的Y,即分类依据数据：\n",
    "<br>列0~12，表明字符索引；行0~11对应padding的长度：12\n",
    "<img src='./image/attentiondatatarget.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (1000, 50)\n",
      "target shape: (1000, 12, 13)\n",
      "(3, 50)\n",
      "(3, 12, 13)\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = './keras_attention/data'\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "human_vocab_file = os.path.join(DATA_FOLDER, 'human_vocab.json')\n",
    "machine_vocab_file = os.path.join(DATA_FOLDER, 'machine_vocab.json')\n",
    "# 因为手写日期格式比较长，如：28 September 2018, 所以设置为50\n",
    "input_vocab = Vocabulary(human_vocab_file, padding=50)\n",
    "# 因为标准日期格式相对较短，如：2018-05-12，所以padding设置为12就可以了\n",
    "output_vocab = Vocabulary(machine_vocab_file, padding=12)\n",
    "datacsv = os.path.join(DATA_FOLDER, 'validation.csv')\n",
    "ds = Data(datacsv, input_vocab, output_vocab)\n",
    "ds.load()\n",
    "ds.transform()\n",
    "print('input shape: {0}'.format(ds.inputs.shape))\n",
    "print('target shape: {0}'.format(ds.targets.shape))\n",
    "# 设置mini-batch为32，通过迭代器的方式返回数据\n",
    "g = ds.generator(32)\n",
    "# print(ds.inputs[0:3])\n",
    "# print(ds.targets[0:3])\n",
    "print(ds.inputs[[5,10, 115]].shape)\n",
    "print(ds.targets[[5,10,12]].shape)\n",
    "# for i in range(50):\n",
    "#     print(next(g)[0].shape)\n",
    "#     print(next(g)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型训练的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型用的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EXAMPLES = ['26th January 2016', '3 April 1989', '5 Dec 09', 'Sat 8 Jun 2017']\n",
    "\n",
    "def run_example(model, input_vocabulary, output_vocabulary, text):\n",
    "    encoded = input_vocabulary.string_to_int(text)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return output_vocabulary.int_to_string(prediction)\n",
    "\n",
    "def run_examples(model, input_vocabulary, output_vocabulary, examples=EXAMPLES):\n",
    "    predicted = []\n",
    "    for example in examples:\n",
    "        print('~~~~~')\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, output_vocabulary, example)))\n",
    "        print('input:',example)\n",
    "        print('output:',predicted[-1])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_FOLDER = './keras_attention/weights'\n",
    "if not os.path.exists(WEIGHT_FOLDER):\n",
    "    os.makedirs(WEIGHT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointfile = os.path.join(WEIGHT_FOLDER, 'NMT.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "cp = ModelCheckpoint(checkpointfile,\n",
    "                     monitor='val_loss',\n",
    "                     verbose=0,\n",
    "                     save_best_only=True,\n",
    "                     save_weights_only=True,\n",
    "                     mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为用到了自定义的AttentionDecoder层，准确度评估也是用自定义的all_acc方法，所以在load_model要加入：\n",
    "```\n",
    "custom_objects={\"AttentionDecoder\":AttentionDecoder,\"all_acc\": all_acc}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincsv = os.path.join(DATA_FOLDER, 'training.csv')\n",
    "validationcsv = os.path.join(DATA_FOLDER, 'validation.csv')\n",
    "def trainapply(gpu=\"0\", \n",
    "               padding=50, \n",
    "               training_data=traincsv, \n",
    "               validation_data=validationcsv,\n",
    "               batch_size=32,\n",
    "               epoch=50):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "    # Dataset functions\n",
    "    input_vocab = Vocabulary(os.path.join(DATA_FOLDER, 'human_vocab.json'), padding=padding)\n",
    "    output_vocab = Vocabulary(os.path.join(DATA_FOLDER, 'machine_vocab.json'),\n",
    "                              padding=padding)\n",
    "    \n",
    "    modelpath = os.path.join(WEIGHT_FOLDER, 'sample_NMT.h5')\n",
    "    if os.path.exists(modelpath):\n",
    "        print('load model: {0} begin'.format(modelpath))\n",
    "        model = load_model(modelpath,\n",
    "                           custom_objects={\n",
    "                               \"AttentionDecoder\":AttentionDecoder,\n",
    "                               \"all_acc\": all_acc})\n",
    "        print('load model: {0} end'.format(modelpath))\n",
    "    else:\n",
    "        print('Loading datasets.')\n",
    "        training = Data(training_data, input_vocab, output_vocab)\n",
    "        validation = Data(validation_data, input_vocab, output_vocab)\n",
    "        training.load()\n",
    "        validation.load()\n",
    "        training.transform()\n",
    "        validation.transform()\n",
    "\n",
    "        print('Datasets Loaded.')\n",
    "        print('Compiling Model.')\n",
    "        model = simpleNMT(pad_length=padding,\n",
    "                          n_chars=input_vocab.size(),\n",
    "                          n_labels=output_vocab.size(),\n",
    "                          embedding_learnable=False,\n",
    "                          encoder_units=256,\n",
    "                          decoder_units=256,\n",
    "                          trainable=True,\n",
    "                          return_probabilities=False)\n",
    "\n",
    "        model.summary()\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy', all_acc])\n",
    "        print('Model Compiled.')\n",
    "        print('Training. Ctrl+C to end early.')\n",
    "\n",
    "        try:\n",
    "            model.fit_generator(generator=training.generator(batch_size),\n",
    "                                steps_per_epoch=100,\n",
    "                                validation_data=validation.generator(batch_size),\n",
    "                                validation_steps=100,\n",
    "                                callbacks=[cp],\n",
    "                                workers=1,\n",
    "                                verbose=1,\n",
    "                                epochs=epoch)\n",
    "            model.save(modelpath)\n",
    "        except KeyboardInterrupt as e:\n",
    "            print('Model training stopped early.')\n",
    "        print('Model training complete.')\n",
    "\n",
    "    run_examples(model, input_vocab, output_vocab,\n",
    "                 examples=['24th December 2018',\n",
    "                           '3 April 1989',\n",
    "                           '5 Dec 09',\n",
    "                           'Sat 8 Jun 2017',\n",
    "                           '21 03 16',\n",
    "                           '21 05 39',\n",
    "                           '23.8.99',\n",
    "                           '26.12.18',\n",
    "                           'TUESDAY, August 23, 1993'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    trainapply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets.\n",
      "Datasets Loaded.\n",
      "Compiling Model.\n",
      "inputs shape: (?, ?, 512)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "OneHot (Embedding)           (None, 50, 60)            3600      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 512)           649216    \n",
      "_________________________________________________________________\n",
      "attention_decoder_1 (Attenti (None, 50, 13)            938934    \n",
      "=================================================================\n",
      "Total params: 1,591,750\n",
      "Trainable params: 1,588,150\n",
      "Non-trainable params: 3,600\n",
      "_________________________________________________________________\n",
      "Model Compiled.\n",
      "Training. Ctrl+C to end early.\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 203s 2s/step - loss: 0.5378 - acc: 0.8373 - all_acc: 0.0000e+00 - val_loss: 0.3383 - val_acc: 0.8788 - val_all_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 199s 2s/step - loss: 0.2971 - acc: 0.8871 - all_acc: 0.0000e+00 - val_loss: 0.2440 - val_acc: 0.8960 - val_all_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 197s 2s/step - loss: 0.2379 - acc: 0.8971 - all_acc: 0.0000e+00 - val_loss: 0.2357 - val_acc: 0.8972 - val_all_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 198s 2s/step - loss: 0.2352 - acc: 0.8980 - all_acc: 0.0000e+00 - val_loss: 0.2364 - val_acc: 0.8972 - val_all_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 207s 2s/step - loss: 0.2349 - acc: 0.8974 - all_acc: 0.0000e+00 - val_loss: 0.2337 - val_acc: 0.8963 - val_all_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 194s 2s/step - loss: 0.2332 - acc: 0.9003 - all_acc: 0.0000e+00 - val_loss: 0.2325 - val_acc: 0.8986 - val_all_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 199s 2s/step - loss: 0.2298 - acc: 0.9023 - all_acc: 0.0000e+00 - val_loss: 0.2265 - val_acc: 0.9042 - val_all_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 226s 2s/step - loss: 0.2251 - acc: 0.9067 - all_acc: 0.0000e+00 - val_loss: 0.2229 - val_acc: 0.9100 - val_all_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 202s 2s/step - loss: 0.2010 - acc: 0.9221 - all_acc: 0.0000e+00 - val_loss: 0.1838 - val_acc: 0.9277 - val_all_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 190s 2s/step - loss: 0.1734 - acc: 0.9324 - all_acc: 0.0000e+00 - val_loss: 0.1691 - val_acc: 0.9365 - val_all_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 191s 2s/step - loss: 0.1618 - acc: 0.9380 - all_acc: 0.0000e+00 - val_loss: 0.1499 - val_acc: 0.9444 - val_all_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 187s 2s/step - loss: 0.1247 - acc: 0.9546 - all_acc: 0.0000e+00 - val_loss: 0.1031 - val_acc: 0.9608 - val_all_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 188s 2s/step - loss: 0.0838 - acc: 0.9694 - all_acc: 0.0000e+00 - val_loss: 0.0673 - val_acc: 0.9770 - val_all_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 192s 2s/step - loss: 0.0489 - acc: 0.9831 - all_acc: 0.0000e+00 - val_loss: 0.0377 - val_acc: 0.9872 - val_all_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 187s 2s/step - loss: 0.0304 - acc: 0.9898 - all_acc: 0.0000e+00 - val_loss: 0.0208 - val_acc: 0.9934 - val_all_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 187s 2s/step - loss: 0.0159 - acc: 0.9953 - all_acc: 0.0000e+00 - val_loss: 0.0110 - val_acc: 0.9973 - val_all_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 188s 2s/step - loss: 0.0105 - acc: 0.9969 - all_acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.9983 - val_all_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 187s 2s/step - loss: 0.0058 - acc: 0.9986 - all_acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.9990 - val_all_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 189s 2s/step - loss: 0.0042 - acc: 0.9989 - all_acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.9995 - val_all_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 200s 2s/step - loss: 0.0031 - acc: 0.9993 - all_acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.9992 - val_all_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 190s 2s/step - loss: 0.0014 - acc: 0.9998 - all_acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.9999 - val_all_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 189s 2s/step - loss: 0.0011 - acc: 0.9998 - all_acc: 0.0000e+00 - val_loss: 7.7439e-04 - val_acc: 0.9999 - val_all_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 197s 2s/step - loss: 6.4591e-04 - acc: 0.9999 - all_acc: 0.0000e+00 - val_loss: 5.3714e-04 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 193s 2s/step - loss: 5.1607e-04 - acc: 0.9999 - all_acc: 0.0000e+00 - val_loss: 4.8462e-04 - val_acc: 0.9999 - val_all_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 190s 2s/step - loss: 4.9230e-04 - acc: 0.9999 - all_acc: 0.0000e+00 - val_loss: 3.6804e-04 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 190s 2s/step - loss: 7.2458e-04 - acc: 0.9998 - all_acc: 0.0000e+00 - val_loss: 0.0060 - val_acc: 0.9982 - val_all_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 188s 2s/step - loss: 0.0031 - acc: 0.9991 - all_acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.9992 - val_all_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 188s 2s/step - loss: 0.0013 - acc: 0.9998 - all_acc: 0.0000e+00 - val_loss: 5.6989e-04 - val_acc: 0.9999 - val_all_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 189s 2s/step - loss: 4.4358e-04 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 3.6360e-04 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 188s 2s/step - loss: 7.6646e-04 - acc: 0.9998 - all_acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.9994 - val_all_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 188s 2s/step - loss: 5.4384e-04 - acc: 0.9999 - all_acc: 0.0000e+00 - val_loss: 1.8371e-04 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 267s 3s/step - loss: 1.3673e-04 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 1.1701e-04 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 295s 3s/step - loss: 1.0138e-04 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 8.6836e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 228s 2s/step - loss: 8.6193e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 7.6808e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 255s 3s/step - loss: 6.8653e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 6.6203e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 241s 2s/step - loss: 6.5755e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 5.8198e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 255s 3s/step - loss: 5.3008e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 5.3308e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 276s 3s/step - loss: 4.8700e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 5.5058e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 245s 2s/step - loss: 0.0017 - acc: 0.9995 - all_acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.9973 - val_all_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "100/100 [==============================] - 244s 2s/step - loss: 0.0032 - acc: 0.9992 - all_acc: 0.0000e+00 - val_loss: 3.5728e-04 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 251s 3s/step - loss: 3.0092e-04 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 2.0551e-04 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 274s 3s/step - loss: 1.0593e-04 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 9.0001e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 239s 2s/step - loss: 7.1180e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 6.7957e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 237s 2s/step - loss: 5.7459e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 5.6911e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 254s 3s/step - loss: 5.0775e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 4.9525e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 257s 3s/step - loss: 4.3338e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 4.2716e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 253s 3s/step - loss: 3.7456e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 3.7250e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 220s 2s/step - loss: 3.2662e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 3.5423e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 228s 2s/step - loss: 2.8877e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 3.0628e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 212s 2s/step - loss: 2.7442e-05 - acc: 1.0000 - all_acc: 0.0000e+00 - val_loss: 2.8302e-05 - val_acc: 1.0000 - val_all_acc: 0.0000e+00\n",
      "Model training complete.\n",
      "~~~~~\n",
      "input: 24th December 2018\n",
      "output: 2018-12-24<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: 3 April 1989\n",
      "output: 1989-04-03<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: 5 Dec 09\n",
      "output: 2009-12-05<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: Sat 8 Jun 2017\n",
      "output: 2017-06-08<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: 21 03 16\n",
      "output: 2016-03-21<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: 21 05 39\n",
      "output: 1999-05-21<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: 23.8.99\n",
      "output: 1999-08-23<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: 26.12.18\n",
      "output: 2018-12-26<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "~~~~~\n",
      "input: TUESDAY, August 23, 1993\n",
      "output: 1993-08-23<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

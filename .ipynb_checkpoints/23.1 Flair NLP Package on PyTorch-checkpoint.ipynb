{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair（PyTorch构建的NLP开发包）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair安装需要Python 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接通过```pip install flair```即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/flairinstall.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair的最主要特性 - 基于预训练模型做迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair的最主要特性，是基于预训练模型做迁移学习。\n",
    "\n",
    "预训练模型的声明，集中在：python安装路径\\Lib\\site-packages\\flair\\embeddings.py中：\n",
    "\n",
    "<img src='./image/flairembeddings.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由模块包含的类可以发现除了基础的WordEmbedding之外，还有最近的新贵ELMo,甚至Bert。\n",
    "\n",
    "值得一提的是，所有支持的预训练模型，都在代码提供了下载地址，除了Bert之外，预训练模型都来自AllenNLP：\n",
    "如：[ELMo_2x_1024_128_2048cnn_1xhighway_options](https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json)\n",
    "\n",
    "Bert相关的预训练模型处理部分，位于：Python安装路径\\Lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\n",
    "<img src='./image/flairbert.png' />\n",
    "\n",
    "其也提供了下载地址，如:[bert-base-uncased](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz)\n",
    "\n",
    "具体的使用部分，可以参考[官方文档](https://github.com/zalandoresearch/flair/tree/master/resources/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair预训练模型的存放路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair相关的目录存放在登录用户的主目录，如:\n",
    "\n",
    "Windows位于C:\\Users\\登录用户名\\\n",
    "\n",
    "Linux对应于~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 除了Bert与Elmo之外的可直接使用模型以及Embedding的存放路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可直接使用模型(.pt扩展名，即pytorch训练的模型)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用法\n",
    "<br>通过TextClassifier加载模型，如：```classifier = TextClassifier.load('en-sentiment')```\n",
    "- 模型种类\n",
    "<br>目前只有两个：\n",
    ">en-sentiment: 情绪识别，基于imdb的英文语料训练\n",
    "<br>de-offensive-language:基于德语的是否有攻击语言判断分类\n",
    "\n",
    "存放于~/.flair/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordEmbeddings，包括：GloVe, EXTVec, Crawl, Twitter, Wiki, \n",
    "- FlairEmbeddings, 包括:\n",
    "<br>multilingual forward (English, German, French, Italian, Dutch, Polish)\n",
    "<br>multilingual backward  (English, German, French, Italian, Dutch, Polish)\n",
    "<br>multilingual forward fast (English, German, French, Italian, Dutch, Polish)\n",
    "<br>multilingual backward fast (English, German, French, Italian, Dutch, Polish)\n",
    "<br>news-english-forward\n",
    "<br>news-english-backward\n",
    "<br>news-english-forward-fast\n",
    "<br>news-english-backward-backward\n",
    "<br>mix-english-forward\n",
    "<br>mix-english-backward\n",
    "<br>mix-german-forward\n",
    "<br>mix-german-backward\n",
    "<br>common crawl Polish forward\n",
    "<br>common crawl Polish backward\n",
    "<br>Slovenian forward\n",
    "<br>Slovenian backward\n",
    "<br>Bulgarian forward\n",
    "<br>Bulgarian backward\n",
    "<br>Dutch forward\n",
    "<br>Dutch backward\n",
    "<br>Swedish forward\n",
    "<br>Swedish backward\n",
    "<br>French forward\n",
    "<br>French backward\n",
    "<br>Czech forward\n",
    "<br>Czech backward\n",
    "<br>Portuguese forward\n",
    "<br>Portuguese backward\n",
    "- CharLMEmbeddings：即将被去除，与FlairEmbeddings作用一样，已经被其取代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存放于~/.flair/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElMo预训练模型的存放路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo预训练的模型包括：small, medium两种，此外还有一个葡萄牙语的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存放于~/.allennlp/cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert预训练模型的存放路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert预训练模型的类型：bert-base-uncased， bert-large-uncased，bert-base-cased，bert-base-multilingual，bert-base-chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存放于~/.pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用训练好的预置分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最新的Flair 0.4版本包含有两个预先训练好的模型。一个基于IMDB数据集训练的情感分析模型和一个攻击性语言探测模型（当前仅支持德语）。\n",
    "\n",
    "只需一个命令就可以下载、存储并使用模型，这使得预置模型的使用过程异常简单。例如，下面的代码将使用情感分析模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当第一次运行以下代码时，Flair将下载情感分析模型，默认情况下会保存到本地用户主目录的.flair子目录，下载可能需要几分钟。（<b>作者太皮了，我下载到本地预估至少用了3个小时！！强烈建议先找到需要下载的预训练模型路径，通过迅雷等工具，下载下来，并拷贝到对应目录，从而缩短等待时间！</b>）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "print('load en-sentiment begin')\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "print('load en-sentiment end')\n",
    "\n",
    "sentence = Sentence('Flair is pretty neat!')\n",
    "print('load predict begin')\n",
    "classifier.predict(sentence)\n",
    "print('load predict end')\n",
    "\n",
    "# print sentence with predicted labels\n",
    "print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码首先载入必要的库，然后载入情感分析模型到内存中（必要时先下载），接下来就可以预测“Flair is pretty neat!”的情感分值了（0~1之间）。最后的命令输入结果为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence above is: [Positive (1.0)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是这么简单！现在你可以将上述代码整合为一个REST API，提供类似于google云端情感分析API的功能了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用训练好的预置分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要训练一个自定义的文本分类器，首先需要一个标注文本集。Flair的分类数据集格式基于Facebook的FastText格式，要求在每一行的开始使用```**label**```前缀定义一个或多个标签。格式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">```__label__<class_1> <text>\n",
    "__label__<class_2> <text>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这篇文章中我们将使用Kaggle的SMS垃圾信息检测数据集来用Flair构建一个垃圾/非垃圾分类器。这个数据集很适合我们的学习任务，因为它很小，只有5572行数据，可以在单个CPU上只花几分钟就完成模型的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理 - 构建数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先下载Kaggle上的[数据集](https://www.kaggle.com/uciml/sms-spam-collection-dataset)，得到spam.csv；然后再数据集目录下，运行我们的处理脚本，得到训练集、开发集和测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# The frac keyword argument specifies the fraction of rows to return in the random sample, \n",
    "# so frac=1 means return all rows (in random order).\n",
    "data = pd.read_csv(\"./csv/spam.csv\", encoding='latin-1').sample(frac=1).drop_duplicates()\n",
    "\n",
    "data = data[['v1', 'v2']].rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    " \n",
    "data['label'] = '__label__' + data['label'].astype(str)\n",
    "\n",
    "data.iloc[0:int(len(data)*0.8)].to_csv('./csv/spamtrain.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('./csv/spamtest.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.9):].to_csv('./csv/spamdev.csv', sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的脚本会进行剔重和随机乱序处理，并按照80/10/10的比例进行数据集的分割。\n",
    "\n",
    "脚本成功执行后，就会得到FastText格式的三个数据文件：train.csv、dev.csv和test.csv。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>如果需要支持多标签分类，标签之间用空格分隔。</b>\n",
    "\n",
    "最简单的方式就是通过```__label__标签1 __label__标签2```实现，如下面所示实现了三标签分类：\n",
    "\n",
    "```__label__add __label__close __label__merge```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练自定义文本分类模型\n",
    "\n",
    "这里使用了WordEmbeddings: GloVe作为预训练模型，不过DocumentLSTMEmbeddings类，是支持Emedding列表的，所以按照如下代码去定义包含Bert预训练模型的embedding列表，然后实例化DocumentLSTMEmbeddings也是可以的：\n",
    "\n",
    "```\n",
    "# BertEmbeddings模型使用base uncase预训练模型\n",
    "word_embeddings = [BertEmbeddings(), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512,reproject_words=True,reproject_words_dimension=256,\n",
    "bidirectional=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ./nlpmodel/flair/spam/best-model.pt existed!\n"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./nlpmodel/flair/spam/best-model.pt'):\n",
    "    corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./csv'), \n",
    "                                                           test_file='spamtrain.csv', \n",
    "                                                           dev_file='spamdev.csv', \n",
    "                                                           train_file='spamtest.csv')\n",
    "\n",
    "    word_embeddings = [WordEmbeddings('glove'), \n",
    "                       FlairEmbeddings('news-forward-fast'), \n",
    "                       FlairEmbeddings('news-backward-fast')]\n",
    "\n",
    "    document_embeddings = DocumentLSTMEmbeddings(word_embeddings, \n",
    "                                                 hidden_size=512, \n",
    "                                                 reproject_words=True, \n",
    "                                                 reproject_words_dimension=256)\n",
    "\n",
    "    classifier = TextClassifier(document_embeddings, \n",
    "                                label_dictionary=corpus.make_label_dictionary(), \n",
    "                                multi_label=False)\n",
    "\n",
    "    trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "    trainer.train('./nlpmodel/flair/spam', max_epochs=20)\n",
    "else:\n",
    "    print('model: {0} existed!'.format('./nlpmodel/flair/spam/best-model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一次运行上面这个脚本时，Flair会自动下载所需要的嵌入模型，这可能需要一段时间，然后接下来的整个训练过程还需要大约5分钟。\n",
    "\n",
    "脚本首先载入需要的库和数据集，得到一个corpus对象。\n",
    "\n",
    "接下来，我们创建一个嵌入列表，包含两个Flair上下文字符串嵌入和一个GloVe单词嵌入，这个列表接下来将作为我们文档嵌入对象的输入。\n",
    "\n",
    "堆叠和文本嵌入是Flair中最有趣的感念之一，它们提供了将不同的嵌入整合在一起的手段，你可以同时使用传统的单词嵌入（例如GloVe、word2vector、ELMo）和Flair的上下文字符串嵌入。\n",
    "\n",
    "在上面的示例中我们使用一个基于LSTM的方法来生成文档嵌入，关于该方法的详细描述可以参考[这里](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面进行简单的垃圾信息预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hi. Yes mum, I will... is belong to: [ham (1.0)]\n",
      "Text: Want 2 get laid tonight? Want real Dogging locations sent direct 2 ur mob? Join the UK's largest Dogging Network bt Txting GRAVEL to 69888! Nt. ec2a. 31p.msg@150p is belong to: [spam (1.0)]\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load_from_file('./nlpmodel/flair/spam/best-model.pt')\n",
    "textlist= [\"Hi. Yes mum, I will...\", \n",
    "           \"Want 2 get laid tonight? Want real Dogging locations sent direct 2 ur mob? Join the UK's largest Dogging Network bt Txting GRAVEL to 69888! Nt. ec2a. 31p.msg@150p\"\n",
    "          ]\n",
    "for text in textlist:\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    print('Text: {0} is belong to: {1}'.format(text, sentence.labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，经过加载预处理的GloVe单词嵌入生成的模型非常大，即使是Spam这种只有几千句，492K的训练集，生成的模型，也有200多M！（通过Bert Base预训练模型进行迁移学习，训练得到的模型将近500M）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/flairmodel.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch实践_Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion-MNIST介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在神经网络介绍中，通过CNN对MNIST数据集做了分类实践，MNIST是做什么的呢？\n",
    "\n",
    "经典的 MNIST 数据集包含了大量的手写数字。十几年来，来自机器学习、机器视觉、人工智能、深度学习领域的研究员们把这个数据集作为衡量算法的基准之一。你会在很多的会议，期刊的论文中发现这个数据集的身影。实际上，MNIST 数据集已经成为算法作者的必测的数据集之一。有人曾调侃道：\" 如果一个算法在 MNIST 不 work, 那么它就根本没法用；而如果它在 MNIST 上 work, 它在其他数据上也可能不 work！\"\n",
    "\n",
    "<img src='./image/mnist.png' />\n",
    "\n",
    "但是对于MNIST的诟病也有很多：\n",
    "\n",
    ">MNIST 太简单了，很多算法在测试集上的性能已经达到 99.6%！不妨看看我们基于 scikit-learn 上对经典机器学习算法的评测 和这段代码： \"Most pairs of MNIST digits can be distinguished pretty well by just one pixel\"（翻译：大多数 MNIST 只需要一个像素就可以区分开！）\n",
    "\n",
    ">MNIST 被用烂了。参考：\"Ian Goodfellow wants people to move away from mnist\"（翻译：Ian Goodfellow 希望人们不要再用 MNIST 了。）\n",
    "\n",
    ">MNIST 数字识别的任务不代表现代机器学习。参考：\"Fran ç ois Cholle: Ideas on MNIST do not transfer to real CV\" （翻译：在 MNIST 上看似有效的想法没法迁移到真正的机器视觉问题上。）\n",
    "\n",
    "FashionMNIST 是一个替代 MNIST 手写数字集的图像数据集。 它是由 Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自 10 种类别的共 7 万个不同商品的正面图片。FashionMNIST 的大小、格式和训练集 / 测试集划分与原始的 MNIST 完全一致。60000/10000 的训练测试数据划分，28x28 的灰度图片。你可以直接用它来测试你的机器学习和深度学习算法性能，且不需要改动任何的代码。\n",
    "\n",
    "简单来说：\n",
    "Fashion-MNIST 的目的是要成为 MNIST 数据集的一个直接替代品。作为算法作者，你不需要修改任何的代码，就可以直接使用这个数据集。Fashion-MNIST 的图片大小，训练、测试样本数及类别数与经典 MNIST 完全相同。\n",
    "\n",
    "相关文章：[连 LeCun 都推荐的 Fashion-MNIST 数据集，是这位华人博士的成果](http://app.myzaker.com/news/article.php?pk=59a3e2ebd1f149c0130000fb)\n",
    "\n",
    "具体GitHub的链接：[fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\n",
    "\n",
    "Fashion-MNIST的示意图：\n",
    "<img src='./image/fashion-mnist-sprite.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关文件下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其GitHub有下载地址：\n",
    "<img src='./image/fmdl.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这四个文件下载下来，并解压，得到4个没有扩展名的二进制文件，train开头的文件是训练集，t10k开头的文件是测试集。\n",
    "\n",
    "一会我们将图像以及分类标签，从这四个文件拆分出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST的标签如下：\n",
    "<img src='./image/fmlabel.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后，将下载的fashion-mnist的压缩包，解压到合适目录，如```./pytorchdata_fashionmnist```：\n",
    "<img src='./image/fmpath.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中没有扩展名的二进制文件是我们的训练集以及测试集；\n",
    "\n",
    "train与test文件夹是提取的图片文件保存路径，每个文件都是```28*28```的图片\n",
    "\n",
    "train.txt与test.txt是图片与label的mapping信息，如：\n",
    "\n",
    "```\n",
    "./data/train/0.jpg 9\n",
    "./data/train/1.jpg 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过PyTorch训练CNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们开工，通过PyTorch制作一个CNN网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入需要的组件包\n",
    "\n",
    "skimage的目的，是将fashion-mnist的图片二进制数据，保存为图片文件到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "import torchvision.datasets.mnist as mnist\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置fashion-mnist的相关数据文件目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './pytorchdata_fashionmnist/'\n",
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义图像转换函数\n",
    "\n",
    "- 图像二进制与图像分类标签，是配对的:\n",
    "<br>训练集文件为：train-image-idx3-ubyte与train-labels-idx1-ubyte\n",
    "<br>测试集文件为：t10k-image-idx3-ubyte与t10k-labels-idx1-ubyte\n",
    "\n",
    "- 训练集有60000张图片\n",
    "\n",
    "- 测试集有10000张图片\n",
    "\n",
    "- 通过mnist.read_image_file函数，读取图像二进制数据\n",
    "\n",
    "- 通过mnist.read_label_file函数，读取标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_img():\n",
    "    convertdict = {'train': 'train.txt_train', 'test': 'test.txt_t10k'}\n",
    "    for key, value in convertdict.items():\n",
    "        txtfile = value.split('_')[0]\n",
    "        if os.path.exists(os.path.join(root, txtfile)):\n",
    "            print('{0} data has been stored!'.format(key))\n",
    "            continue\n",
    "        imagepre = value.split('_')[1]\n",
    "        dataset = (mnist.read_image_file(os.path.join(root,\n",
    "                                                      '{0}-images-idx3-ubyte'.format(imagepre))),\n",
    "                   mnist.read_label_file(os.path.join(root,\n",
    "                                                      '{0}-labels-idx1-ubyte'.format(imagepre))))\n",
    "        print('{0} set size: {1}'.format(key, dataset[0].size()))\n",
    "        with open(os.path.join(root, txtfile), 'w', encoding='utf-8') as file:\n",
    "            data_path = os.path.join(root, key)\n",
    "            if not os.path.exists(data_path):\n",
    "                os.makedirs(data_path)\n",
    "            print('Save {0} image file begin'.format(key))\n",
    "            for i, (img, label) in enumerate(zip(dataset[0], dataset[1])):\n",
    "                print('save image file: {0}'.format(i + 1))\n",
    "                img_path = os.path.join(data_path,\n",
    "                                        str(i) + '.jpg').replace('\\\\', '/')\n",
    "                io.imsave(img_path, img.numpy())\n",
    "                labelnumlist = re.findall(r'\\d+', str(label))\n",
    "                if len(labelnumlist) > 0:\n",
    "                    file.write(img_path + ' ' + str(labelnumlist[0]) + '\\n')\n",
    "            print('Save {0} image file end'.format(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过PIL的Image模块，将图片加载并转为RGB三通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继承torch.utils.data中的Dataset类，作为定制化的数据集生成类，其中target_transform一般为torchvision包的transforms模块的ToTensor类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, transform=None, target_transform=None, loader=default_loader):\n",
    "        with open(txt, 'r', encoding='utf-8') as fh:\n",
    "            imgs = []\n",
    "            for line in fh:\n",
    "                line = line.strip('\\n')\n",
    "                line = line.rstrip()\n",
    "                words = line.split()\n",
    "                imgs.append((words[0], int(words[1])))\n",
    "            self.imgs = imgs\n",
    "            self.transform = transform\n",
    "            self.target_transform = target_transform\n",
    "            self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.imgs[index]\n",
    "        img = self.loader(fn)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义CNN神经网络\n",
    "\n",
    "- 继承自torch.nn.Module\n",
    "\n",
    "- 共3个卷积层，每个卷积层都有窗口大小与步长为2的池化层，使用ReLU作为卷积层激活函数\n",
    "\n",
    "- 一个输出层，进行10分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.dense = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64 * 3 * 3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv3_out = self.conv3(conv2_out)\n",
    "        res = conv3_out.view(conv3_out.size(0), -1)\n",
    "        out = self.dense(res)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch可以通过torch.cuda.is_available()的函数判断当前系统是否支持GPU，从而可以自适应选择GPU或CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化模型的方法\n",
    "\n",
    "- ```mymodel.to(device)```, 这一句即做到模型训练自适应（选择GPU或CPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialmodel():\n",
    "    mymodel = Net()\n",
    "    mymodel = mymodel.to(device)\n",
    "    if device == 'cuda':\n",
    "        mymodel = torch.nn.DataParallel(mymodel)\n",
    "        cudnn.benchmark = True\n",
    "    return mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- epochcount，代表训练多少次\n",
    "- resume，代表是否接着之前训练完毕的模型继续训练\n",
    "- 首先通过MyDataset类，加载训练集，并且设置transforms.ToTensor()作为transform方法\n",
    "- PyTorch要求必须按照mini-batch的方式进行训练\n",
    "- 通过DataLoader加载训练集，设置每个批次为64条数据，shuffle=True，代表将训练集随机打散\n",
    "- 优化函数，选择Adam (torch.optim.Adam)\n",
    "- 损失函数，选择交叉熵 (torch.nn.CrossEntropyLoss)\n",
    "- 必须通过model变量.train()的方式，声明：要开始训练了\n",
    "- 前向传播结束之后，需要通过optimizer.zero_grad()，是把梯度置零，也就是把loss关于weight的导数变成0。\n",
    ">原因：根据pytorch中的backward()函数的计算，当网络参量进行反馈时，梯度是被积累的而不是被替换掉；但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积，因此这里就需要每个batch设置一遍zero_grad 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochcount=10, resume=True):\n",
    "    train_data = MyDataset(txt=os.path.join(root, 'train.txt'), transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "    startepoch = 0\n",
    "    if resume:\n",
    "        mymodel, startepoch = loadmodel()\n",
    "        if startepoch >= epochcount:\n",
    "            print('The epoch of trained model is {0}, greater than epoch count: {1}, need not train it!'.\n",
    "                  format(startepoch, epochcount))\n",
    "    else:\n",
    "        mymodel = initialmodel()\n",
    "    print(mymodel)\n",
    "    # 优化函数，选择Adam\n",
    "    optimizer = torch.optim.Adam(mymodel.parameters())\n",
    "    # 损失函数，选择交叉熵\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    # 设置checkpoint的模型存放路径\n",
    "    modelfolder = os.path.join(os.path.join(root, 'model'))\n",
    "    if not os.path.exists(modelfolder):\n",
    "        os.makedirs(modelfolder)\n",
    "    modelfile = os.path.join(modelfolder, 'fashionmnist.pt')\n",
    "    for epoch in range(startepoch, epochcount):\n",
    "        print('epoch {}'.format(epoch + 1))\n",
    "        # 必须通过model变量.train()的方式，声明：要开始训练了\n",
    "        mymodel.train()\n",
    "        # training-----------------------------\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        best_acc = 0\n",
    "        for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            print('Train epoch:{0} iter: {1} begin'.format(epoch, i + 1))\n",
    "            # 自适应训练数据是否支持GPU\n",
    "            batch_x, batch_y = Variable(batch_x.to(device)), Variable(batch_y.to(device))\n",
    "            out = mymodel(batch_x)\n",
    "            # 动态更新loss与accuracy的数量\n",
    "            loss = loss_func(out, batch_y)\n",
    "            train_loss += loss.data.item()\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            # 通过torch.sum统计预测正确的数量\n",
    "            train_correct = torch.sum(pred == batch_y)\n",
    "            # train_correct是Tensor类型，需要通过item()，得到数值\n",
    "            train_acc += train_correct.item()\n",
    "            # optimizer.zero_grad()意思是把梯度置零，也就是把loss关于weight的导数变成0\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('Train epoch:{0} iter: {1} end'.format(epoch, i + 1))\n",
    "        accrate = train_acc / (len(train_data))\n",
    "        # 输出每个epoch的损失率与准确率\n",
    "        print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(train_data)), accrate))\n",
    "        # 如果当前epoch的准确率是最高的，则保存模型。\n",
    "        # 模型的权重信息是通过state_dict()获得的，这也是模型的最关键信息\n",
    "        if accrate > best_acc:\n",
    "            print('save model for best accuate: {0} begin'.format(accrate))\n",
    "            best_acc = accrate\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': mymodel.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }, modelfile)\n",
    "            print('save model for best accuate: {0} end'.format(accrate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型\n",
    "- 如果模型不存在，则返回初始化的模型\n",
    "- 否则，加载模型，主要是加载state_dict信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel():\n",
    "    modelfolder = os.path.join(os.path.join(root, 'model'))\n",
    "    modelfile = os.path.join(modelfolder, 'fashionmnist.pt')\n",
    "    if not os.path.exists(modelfile):\n",
    "        print('model not exist!')\n",
    "        return initialmodel(), 0\n",
    "    print('Loading best model: {0} begin'.format(modelfile))\n",
    "    mymodel = initialmodel()\n",
    "    checkpoint = torch.load(modelfile)\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    mymodel.load_state_dict(checkpoint['state_dict'])\n",
    "    print('Loading best model end, epoch is {0}, training accuracy is {1}'.format(epoch, best_acc))\n",
    "    return mymodel, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型\n",
    "\n",
    "- 测试集加载与训练集加载方式是一样的\n",
    "- 必须调用model变量.evel()方法，以便在运行推断之前将dropout和batch规范化层设置为评估模式。如果不这样做，将会产生不一致的推断结果。\n",
    "- 评估过程中需要加入：with torch.no_grad()，适用于推断阶段，不需要反向传播\n",
    "- 评估的过程与训练过程自评估方式是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_data = MyDataset(txt=os.path.join(root, 'test.txt'), transform=transforms.ToTensor())\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=64)\n",
    "    mymodel, epoch = loadmodel()\n",
    "    if epoch == 0:\n",
    "        print('Model not be trainned!')\n",
    "        return\n",
    "    # 您必须调用model.eval()，\n",
    "    # 以便在运行推断之前将dropout和batch规范化层设置为评估模式。\n",
    "    # 如果不这样做，将会产生不一致的推断结果。\n",
    "    mymodel.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    # 适用于推断阶段，不需要反向传播\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y) in enumerate(test_loader):\n",
    "            print('Validate iter: {0} begin'.format(i + 1))\n",
    "            batch_x, batch_y = Variable(batch_x.to(device)), Variable(batch_y.to(device))\n",
    "            out = mymodel(batch_x)\n",
    "            loss = loss_func(out, batch_y)\n",
    "            eval_loss += loss.data.item()\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = torch.sum(pred == batch_y)\n",
    "            eval_acc += num_correct.item()\n",
    "            print('Validate iter: {0} end'.format(i + 1))\n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(test_data)), eval_acc / (len(test_data))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试用户自己的图片文件\n",
    "- 这种使用方式，即模拟真实业务场景的数据分类预测\n",
    "- torch.unsqueeze()，因为PyTorch训练模式只有mini-batch，所以哪怕对用户的某个图像进行预测，依然需要将其作为一个数量为1的batch，即将单张图片的：```[3, 28, 28]```转为```[1, 3, 28, 28]```。代码中有更详细的注释\n",
    ">torch.unsqueeze(),这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateuserdata():\n",
    "    imgfile = os.path.join(root, 'train/80.jpg')\n",
    "    category = 1\n",
    "    image = Image.open(imgfile)\n",
    "    plt.figure(\"fashion-mnist\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    imgloader = image.convert('RGB')\n",
    "    img = transforms.ToTensor()(imgloader)\n",
    "    valdata = DataLoader(dataset=img, batch_size=64)\n",
    "    print('load image file: {0}, category: {1} success'.format(imgfile, category))\n",
    "    mymodel, epoch = loadmodel()\n",
    "    mymodel.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        datax = Variable(img.to(device))\n",
    "        # 如果直接将datax，作为参数去预测分类，会遇到如下错误：\n",
    "        # RuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 3, 3, 3],\n",
    "        # but got 3-dimensional input of size [3, 28, 28] instead\n",
    "        # 因为torch.nn仅仅支持mini-batches, 预测的输入数据应该符合: (batch_size, channels, height, width)\n",
    "        # 上述异常，是遗失batch dimension引起的。\n",
    "        # 最简单的方法是通过：datax.unsqueeze(0)，即使其变成[1, 3, 28, 28]的4-dimensions数据\n",
    "        print('Shape before Transorm: {0}'.format(datax.size()))\n",
    "        print('Channel: {0}, Width: {1}, Height: {2}'.format(datax.size()[0], datax.size()[1], datax.size()[2]))\n",
    "        datax = datax.unsqueeze(0)\n",
    "        print('Shape after Transorm: {0}'.format(datax.size()))\n",
    "        out = mymodel(datax)\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        print(pred, pred.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际操作：包括提炼图片，训练模型，对测试集进行验证以及预测单个用户图片分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert image begin\n",
      "train data has been stored!\n",
      "test data has been stored!\n",
      "Convert image end\n",
      "\n",
      "Train model begin\n",
      "Loading best model: ./pytorchdata_fashionmnist/model\\fashionmnist.pt begin\n",
      "Loading best model end, epoch is 10, training accuracy is 0.9427666666666666\n",
      "The epoch of trained model is 10, greater than epoch count: 10, need not train it!\n",
      "Net(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Train model end\n",
      "\n",
      "Test model begin\n",
      "Loading best model: ./pytorchdata_fashionmnist/model\\fashionmnist.pt begin\n",
      "Loading best model end, epoch is 10, training accuracy is 0.9427666666666666\n",
      "Validate iter: 1 begin\n",
      "Validate iter: 1 end\n",
      "Validate iter: 2 begin\n",
      "Validate iter: 2 end\n",
      "Validate iter: 3 begin\n",
      "Validate iter: 3 end\n",
      "Validate iter: 4 begin\n",
      "Validate iter: 4 end\n",
      "Validate iter: 5 begin\n",
      "Validate iter: 5 end\n",
      "Validate iter: 6 begin\n",
      "Validate iter: 6 end\n",
      "Validate iter: 7 begin\n",
      "Validate iter: 7 end\n",
      "Validate iter: 8 begin\n",
      "Validate iter: 8 end\n",
      "Validate iter: 9 begin\n",
      "Validate iter: 9 end\n",
      "Validate iter: 10 begin\n",
      "Validate iter: 10 end\n",
      "Validate iter: 11 begin\n",
      "Validate iter: 11 end\n",
      "Validate iter: 12 begin\n",
      "Validate iter: 12 end\n",
      "Validate iter: 13 begin\n",
      "Validate iter: 13 end\n",
      "Validate iter: 14 begin\n",
      "Validate iter: 14 end\n",
      "Validate iter: 15 begin\n",
      "Validate iter: 15 end\n",
      "Validate iter: 16 begin\n",
      "Validate iter: 16 end\n",
      "Validate iter: 17 begin\n",
      "Validate iter: 17 end\n",
      "Validate iter: 18 begin\n",
      "Validate iter: 18 end\n",
      "Validate iter: 19 begin\n",
      "Validate iter: 19 end\n",
      "Validate iter: 20 begin\n",
      "Validate iter: 20 end\n",
      "Validate iter: 21 begin\n",
      "Validate iter: 21 end\n",
      "Validate iter: 22 begin\n",
      "Validate iter: 22 end\n",
      "Validate iter: 23 begin\n",
      "Validate iter: 23 end\n",
      "Validate iter: 24 begin\n",
      "Validate iter: 24 end\n",
      "Validate iter: 25 begin\n",
      "Validate iter: 25 end\n",
      "Validate iter: 26 begin\n",
      "Validate iter: 26 end\n",
      "Validate iter: 27 begin\n",
      "Validate iter: 27 end\n",
      "Validate iter: 28 begin\n",
      "Validate iter: 28 end\n",
      "Validate iter: 29 begin\n",
      "Validate iter: 29 end\n",
      "Validate iter: 30 begin\n",
      "Validate iter: 30 end\n",
      "Validate iter: 31 begin\n",
      "Validate iter: 31 end\n",
      "Validate iter: 32 begin\n",
      "Validate iter: 32 end\n",
      "Validate iter: 33 begin\n",
      "Validate iter: 33 end\n",
      "Validate iter: 34 begin\n",
      "Validate iter: 34 end\n",
      "Validate iter: 35 begin\n",
      "Validate iter: 35 end\n",
      "Validate iter: 36 begin\n",
      "Validate iter: 36 end\n",
      "Validate iter: 37 begin\n",
      "Validate iter: 37 end\n",
      "Validate iter: 38 begin\n",
      "Validate iter: 38 end\n",
      "Validate iter: 39 begin\n",
      "Validate iter: 39 end\n",
      "Validate iter: 40 begin\n",
      "Validate iter: 40 end\n",
      "Validate iter: 41 begin\n",
      "Validate iter: 41 end\n",
      "Validate iter: 42 begin\n",
      "Validate iter: 42 end\n",
      "Validate iter: 43 begin\n",
      "Validate iter: 43 end\n",
      "Validate iter: 44 begin\n",
      "Validate iter: 44 end\n",
      "Validate iter: 45 begin\n",
      "Validate iter: 45 end\n",
      "Validate iter: 46 begin\n",
      "Validate iter: 46 end\n",
      "Validate iter: 47 begin\n",
      "Validate iter: 47 end\n",
      "Validate iter: 48 begin\n",
      "Validate iter: 48 end\n",
      "Validate iter: 49 begin\n",
      "Validate iter: 49 end\n",
      "Validate iter: 50 begin\n",
      "Validate iter: 50 end\n",
      "Validate iter: 51 begin\n",
      "Validate iter: 51 end\n",
      "Validate iter: 52 begin\n",
      "Validate iter: 52 end\n",
      "Validate iter: 53 begin\n",
      "Validate iter: 53 end\n",
      "Validate iter: 54 begin\n",
      "Validate iter: 54 end\n",
      "Validate iter: 55 begin\n",
      "Validate iter: 55 end\n",
      "Validate iter: 56 begin\n",
      "Validate iter: 56 end\n",
      "Validate iter: 57 begin\n",
      "Validate iter: 57 end\n",
      "Validate iter: 58 begin\n",
      "Validate iter: 58 end\n",
      "Validate iter: 59 begin\n",
      "Validate iter: 59 end\n",
      "Validate iter: 60 begin\n",
      "Validate iter: 60 end\n",
      "Validate iter: 61 begin\n",
      "Validate iter: 61 end\n",
      "Validate iter: 62 begin\n",
      "Validate iter: 62 end\n",
      "Validate iter: 63 begin\n",
      "Validate iter: 63 end\n",
      "Validate iter: 64 begin\n",
      "Validate iter: 64 end\n",
      "Validate iter: 65 begin\n",
      "Validate iter: 65 end\n",
      "Validate iter: 66 begin\n",
      "Validate iter: 66 end\n",
      "Validate iter: 67 begin\n",
      "Validate iter: 67 end\n",
      "Validate iter: 68 begin\n",
      "Validate iter: 68 end\n",
      "Validate iter: 69 begin\n",
      "Validate iter: 69 end\n",
      "Validate iter: 70 begin\n",
      "Validate iter: 70 end\n",
      "Validate iter: 71 begin\n",
      "Validate iter: 71 end\n",
      "Validate iter: 72 begin\n",
      "Validate iter: 72 end\n",
      "Validate iter: 73 begin\n",
      "Validate iter: 73 end\n",
      "Validate iter: 74 begin\n",
      "Validate iter: 74 end\n",
      "Validate iter: 75 begin\n",
      "Validate iter: 75 end\n",
      "Validate iter: 76 begin\n",
      "Validate iter: 76 end\n",
      "Validate iter: 77 begin\n",
      "Validate iter: 77 end\n",
      "Validate iter: 78 begin\n",
      "Validate iter: 78 end\n",
      "Validate iter: 79 begin\n",
      "Validate iter: 79 end\n",
      "Validate iter: 80 begin\n",
      "Validate iter: 80 end\n",
      "Validate iter: 81 begin\n",
      "Validate iter: 81 end\n",
      "Validate iter: 82 begin\n",
      "Validate iter: 82 end\n",
      "Validate iter: 83 begin\n",
      "Validate iter: 83 end\n",
      "Validate iter: 84 begin\n",
      "Validate iter: 84 end\n",
      "Validate iter: 85 begin\n",
      "Validate iter: 85 end\n",
      "Validate iter: 86 begin\n",
      "Validate iter: 86 end\n",
      "Validate iter: 87 begin\n",
      "Validate iter: 87 end\n",
      "Validate iter: 88 begin\n",
      "Validate iter: 88 end\n",
      "Validate iter: 89 begin\n",
      "Validate iter: 89 end\n",
      "Validate iter: 90 begin\n",
      "Validate iter: 90 end\n",
      "Validate iter: 91 begin\n",
      "Validate iter: 91 end\n",
      "Validate iter: 92 begin\n",
      "Validate iter: 92 end\n",
      "Validate iter: 93 begin\n",
      "Validate iter: 93 end\n",
      "Validate iter: 94 begin\n",
      "Validate iter: 94 end\n",
      "Validate iter: 95 begin\n",
      "Validate iter: 95 end\n",
      "Validate iter: 96 begin\n",
      "Validate iter: 96 end\n",
      "Validate iter: 97 begin\n",
      "Validate iter: 97 end\n",
      "Validate iter: 98 begin\n",
      "Validate iter: 98 end\n",
      "Validate iter: 99 begin\n",
      "Validate iter: 99 end\n",
      "Validate iter: 100 begin\n",
      "Validate iter: 100 end\n",
      "Validate iter: 101 begin\n",
      "Validate iter: 101 end\n",
      "Validate iter: 102 begin\n",
      "Validate iter: 102 end\n",
      "Validate iter: 103 begin\n",
      "Validate iter: 103 end\n",
      "Validate iter: 104 begin\n",
      "Validate iter: 104 end\n",
      "Validate iter: 105 begin\n",
      "Validate iter: 105 end\n",
      "Validate iter: 106 begin\n",
      "Validate iter: 106 end\n",
      "Validate iter: 107 begin\n",
      "Validate iter: 107 end\n",
      "Validate iter: 108 begin\n",
      "Validate iter: 108 end\n",
      "Validate iter: 109 begin\n",
      "Validate iter: 109 end\n",
      "Validate iter: 110 begin\n",
      "Validate iter: 110 end\n",
      "Validate iter: 111 begin\n",
      "Validate iter: 111 end\n",
      "Validate iter: 112 begin\n",
      "Validate iter: 112 end\n",
      "Validate iter: 113 begin\n",
      "Validate iter: 113 end\n",
      "Validate iter: 114 begin\n",
      "Validate iter: 114 end\n",
      "Validate iter: 115 begin\n",
      "Validate iter: 115 end\n",
      "Validate iter: 116 begin\n",
      "Validate iter: 116 end\n",
      "Validate iter: 117 begin\n",
      "Validate iter: 117 end\n",
      "Validate iter: 118 begin\n",
      "Validate iter: 118 end\n",
      "Validate iter: 119 begin\n",
      "Validate iter: 119 end\n",
      "Validate iter: 120 begin\n",
      "Validate iter: 120 end\n",
      "Validate iter: 121 begin\n",
      "Validate iter: 121 end\n",
      "Validate iter: 122 begin\n",
      "Validate iter: 122 end\n",
      "Validate iter: 123 begin\n",
      "Validate iter: 123 end\n",
      "Validate iter: 124 begin\n",
      "Validate iter: 124 end\n",
      "Validate iter: 125 begin\n",
      "Validate iter: 125 end\n",
      "Validate iter: 126 begin\n",
      "Validate iter: 126 end\n",
      "Validate iter: 127 begin\n",
      "Validate iter: 127 end\n",
      "Validate iter: 128 begin\n",
      "Validate iter: 128 end\n",
      "Validate iter: 129 begin\n",
      "Validate iter: 129 end\n",
      "Validate iter: 130 begin\n",
      "Validate iter: 130 end\n",
      "Validate iter: 131 begin\n",
      "Validate iter: 131 end\n",
      "Validate iter: 132 begin\n",
      "Validate iter: 132 end\n",
      "Validate iter: 133 begin\n",
      "Validate iter: 133 end\n",
      "Validate iter: 134 begin\n",
      "Validate iter: 134 end\n",
      "Validate iter: 135 begin\n",
      "Validate iter: 135 end\n",
      "Validate iter: 136 begin\n",
      "Validate iter: 136 end\n",
      "Validate iter: 137 begin\n",
      "Validate iter: 137 end\n",
      "Validate iter: 138 begin\n",
      "Validate iter: 138 end\n",
      "Validate iter: 139 begin\n",
      "Validate iter: 139 end\n",
      "Validate iter: 140 begin\n",
      "Validate iter: 140 end\n",
      "Validate iter: 141 begin\n",
      "Validate iter: 141 end\n",
      "Validate iter: 142 begin\n",
      "Validate iter: 142 end\n",
      "Validate iter: 143 begin\n",
      "Validate iter: 143 end\n",
      "Validate iter: 144 begin\n",
      "Validate iter: 144 end\n",
      "Validate iter: 145 begin\n",
      "Validate iter: 145 end\n",
      "Validate iter: 146 begin\n",
      "Validate iter: 146 end\n",
      "Validate iter: 147 begin\n",
      "Validate iter: 147 end\n",
      "Validate iter: 148 begin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate iter: 148 end\n",
      "Validate iter: 149 begin\n",
      "Validate iter: 149 end\n",
      "Validate iter: 150 begin\n",
      "Validate iter: 150 end\n",
      "Validate iter: 151 begin\n",
      "Validate iter: 151 end\n",
      "Validate iter: 152 begin\n",
      "Validate iter: 152 end\n",
      "Validate iter: 153 begin\n",
      "Validate iter: 153 end\n",
      "Validate iter: 154 begin\n",
      "Validate iter: 154 end\n",
      "Validate iter: 155 begin\n",
      "Validate iter: 155 end\n",
      "Validate iter: 156 begin\n",
      "Validate iter: 156 end\n",
      "Validate iter: 157 begin\n",
      "Validate iter: 157 end\n",
      "Test Loss: 0.003907, Acc: 0.914800\n",
      "Test model end\n",
      "\n",
      "Validate user data begin\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load image file: ./pytorchdata_fashionmnist/train/80.jpg, category: 1 success\n",
      "Loading best model: ./pytorchdata_fashionmnist/model\\fashionmnist.pt begin\n",
      "Loading best model end, epoch is 10, training accuracy is 0.9427666666666666\n",
      "Shape before Transorm: torch.Size([3, 28, 28])\n",
      "Channel: 3, Width: 28, Height: 28\n",
      "Shape after Transorm: torch.Size([1, 3, 28, 28])\n",
      "tensor([1]) 1\n",
      "Validate user data end\n"
     ]
    }
   ],
   "source": [
    "print('Convert image begin')\n",
    "convert_to_img()\n",
    "print('Convert image end')\n",
    "print()\n",
    "print('Train model begin')\n",
    "train(epochcount=10, resume=True)\n",
    "print('Train model end')\n",
    "print()\n",
    "print('Test model begin')\n",
    "test()\n",
    "print('Test model end')\n",
    "print()\n",
    "print('Validate user data begin')\n",
    "validateuserdata()\n",
    "print('Validate user data end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
